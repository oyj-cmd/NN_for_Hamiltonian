{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Test Block Transform ====== \n",
      "Read block from path=/home/muyj/Project/Project_1106_deephe3_example/bismuth_workdir/3_openmx_processed/1. \n",
      "Origin Block Irreps (dim=2888): \t\t1x0e+1x1e+1x0e+1x1e+1x0e+1x1e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x0e+1x1e+1x0e+1x1e+1x0e+1x1e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x0e+1x1e+1x0e+1x1e+1x0e+1x1e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x0e+1x1e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x3e+1x4e+1x5e+1x0e+1x1e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x3e+1x4e+1x5e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x0e+1x1e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x3e+1x4e+1x5e+1x0e+1x1e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x3e+1x4e+1x5e+1x0e+1x1e+1x0e+1x1e+1x0e+1x1e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x0e+1x1e+1x0e+1x1e+1x0e+1x1e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x0e+1x1e+1x0e+1x1e+1x0e+1x1e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x0e+1x1e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x3e+1x4e+1x5e+1x0e+1x1e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x3e+1x4e+1x5e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x2e+1x1e+1x2e+1x3e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x1e+1x2e+1x3e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x0e+1x1e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x3e+1x4e+1x5e+1x0e+1x1e+1x2e+1x3e+1x4e+1x1e+1x0e+1x1e+1x2e+1x1e+1x2e+1x3e+1x2e+1x3e+1x4e+1x3e+1x4e+1x5e. \n",
      "Simplified Sorted Block Irreps (dim=2888): \t90x0e+202x1e+192x2e+112x3e+40x4e+8x5e. \n",
      "Hamiltonian Block Shape: torch.Size([1468, 2888]) \n",
      "\n",
      "Is e3nn Sort Block Transform Correct? True.\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from modelname.graph import read_block_irreps_from_one_OpenMX_folder\n",
    "from modelname.graph import old_0_read_label_dict_from_one_OpenMX_folder\n",
    "\n",
    "# is_spin=True in the given example\n",
    "IS_SPIN: bool = True\n",
    "DEFAULT_DTYPE_TORCH = torch.float32\n",
    "FOLDER: str = r\"/home/muyj/Project/Project_1106_deephe3_example/bismuth_workdir/3_openmx_processed/1\"\n",
    "block_irreps_dict, simple_sorted_block_irreps_dict, sort_dict, inv_sort_dict = read_block_irreps_from_one_OpenMX_folder(\n",
    "    FOLDER, IS_SPIN\n",
    ")\n",
    "block_dict_H = old_0_read_label_dict_from_one_OpenMX_folder(\n",
    "    FOLDER, IS_SPIN, DEFAULT_DTYPE_TORCH\n",
    ")\n",
    "# only Bi-Bi pair type contains in the given example\n",
    "PAIR_TYPE: Tuple[int, int] = (83, 83)\n",
    "block_irreps = block_irreps_dict[PAIR_TYPE]\n",
    "simple_irreps = simple_sorted_block_irreps_dict[PAIR_TYPE]\n",
    "sort_id = sort_dict[PAIR_TYPE]\n",
    "inv_id = inv_sort_dict[PAIR_TYPE]\n",
    "h_block = block_dict_H[PAIR_TYPE] # 'h_block' is east-typing\n",
    "print(f\"====== Test Block Transform ====== \\n\"\n",
    "    f\"Read block from path={FOLDER}. \\n\"\n",
    "    f\"Origin Block Irreps (dim={block_irreps.dim}): \\t\\t{block_irreps}. \\n\"\n",
    "    f\"Simplified Sorted Block Irreps (dim={simple_irreps.dim}): \\t{simple_irreps}. \\n\"\n",
    "    f\"Hamiltonian Block Shape: {h_block.shape} \\n\"\n",
    "    )\n",
    "\n",
    "# = Test Sort Block Transform =\n",
    "sort_h_block = h_block[:, inv_id]\n",
    "revert_h_block = sort_h_block[:, sort_id]\n",
    "error = revert_h_block - h_block\n",
    "is_correct: bool = bool((error == 0).all())\n",
    "print(f\"Is e3nn Sort Block Transform Correct? {is_correct}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== Test read_Ls_dict_from_one_OpenMX_folder/read_Ls_dict_from_OpenMX from graph.py ====== \n",
      "Ls_dict from one folder: {83: [0, 0, 0, 1, 1, 2, 2]}\n",
      "Ls_dict from 111 folders: {83: [0, 0, 0, 1, 1, 2, 2]}\n",
      "\n",
      "\n",
      "====== Test revert_blocks_dict/old_0_read_label_dict_from_one_OpenMX_folder/read_label_dict_from_one_OpenMX_folder from graph.py ====== \n",
      "Origin Hamiltonian Block (from hamiltonians.h5) Element Dtype: torch.float32 \n",
      "Transformed Hamiltonian Block (from hamiltonians.h5) Element Dtype: torch.float32 \n",
      " Reverted Hamiltonian Block (from hamiltonians.h5) Element Dtype: torch.float32 \n",
      "Origin Hamiltonian Block (from hamiltonians.h5) Shape: torch.Size([1468, 2, 4, 19, 19]) \n",
      "Transformed Hamiltonian Block (from hamiltonians.h5) Shape: torch.Size([1468, 2888]) \n",
      " Reverted Hamiltonian Block (from hamiltonians.h5) Shape: torch.Size([1468, 2, 4, 19, 19]) \n",
      "\n",
      "\n",
      "\n",
      "====== Test TensorProduct-DirectSum Block Transform ====== \n",
      "Is Test TensorProduct-DirectSum Block Transform Correct? False.\n",
      "Max Error: 9.5367431640625e-06.\n",
      "Mean Error: 2.956402234133293e-09.\n",
      "(Min Absolutet of Origin Hamiltonian Block: 6.488426797982938e-25\n",
      "Min Absolute of Reverted Hamiltonian Block: 5.225261202737917e-24\n",
      "Mean Absolute Value Origin Hamiltonian Block: 0.04179561883211136\n",
      "Mean Absolute Value of Reverted Hamiltonian Block: 0.04179561510682106)\n",
      "Max Relative Error: 4.121e+10.\n",
      "\n",
      "Summary: Nearly Correct, L1Loss: 2.956402234133293e-09 hatree.\n",
      "\n",
      "\n",
      "====== Test revert_blocks_dict from graph.py ====== \n",
      "Just for test. Error is supposed to be large. L1Loss: 0.08032244443893433 hatree.\n"
     ]
    }
   ],
   "source": [
    "# = Other Test for graph.py =\n",
    "import os\n",
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "from modelname.graph import read_Ls_dict_from_one_OpenMX_folder\n",
    "from modelname.graph import read_Ls_dict_from_OpenMX\n",
    "from modelname.graph import old_0_revert_blocks_dict\n",
    "from modelname.graph import old_0_read_label_dict_from_one_OpenMX_folder\n",
    "from modelname.graph import read_label_dict_from_one_OpenMX_folder\n",
    "\n",
    "from modelname.graph import revert_blocks_dict\n",
    "from modelname.graph import read_block_irreps_from_one_OpenMX_folder\n",
    "ROOT: str = r\"/home/muyj/Project/Project_1106_deephe3_example/bismuth_workdir/3_openmx_processed\"\n",
    "FOLDER: str = r\"/home/muyj/Project/Project_1106_deephe3_example/bismuth_workdir/3_openmx_processed/1\"\n",
    "IS_SPIN: bool = True\n",
    "DEFAULT_DTYPE_TORCH = torch.float32\n",
    "PAIR_TYPE: Tuple[int, int] = (83, 83)\n",
    "folder_list: List[str] = []\n",
    "for root, dirs, files in os.walk(ROOT):\n",
    "    if {'element.dat', 'orbital_types.dat', 'lat.dat', 'site_positions.dat'}.issubset(files):\n",
    "        folder_list.append(root)\n",
    "\n",
    "print(\"\\n\\n====== Test read_Ls_dict_from_one_OpenMX_folder/read_Ls_dict_from_OpenMX from graph.py ====== \")\n",
    "Ls_dict: Dict[int, List[int]] = read_Ls_dict_from_one_OpenMX_folder(folder=FOLDER)\n",
    "print(f\"Ls_dict from one folder: {Ls_dict}\")\n",
    "Ls_dict: Dict[int, List[int]] = read_Ls_dict_from_OpenMX(folder_list=folder_list)\n",
    "print(f\"Ls_dict from {len(folder_list)} folders: {Ls_dict}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n====== Test revert_blocks_dict/old_0_read_label_dict_from_one_OpenMX_folder/read_label_dict_from_one_OpenMX_folder from graph.py ====== \")\n",
    "origin_H_dict = read_label_dict_from_one_OpenMX_folder(FOLDER, IS_SPIN, DEFAULT_DTYPE_TORCH)\n",
    "transformed_H_dict = old_0_read_label_dict_from_one_OpenMX_folder(FOLDER, IS_SPIN, DEFAULT_DTYPE_TORCH)\n",
    "revert_H_dict = old_0_revert_blocks_dict(out_dict=transformed_H_dict, \n",
    "                                   Ls_dict=Ls_dict, is_spin=IS_SPIN, default_dtype_torch=DEFAULT_DTYPE_TORCH)\n",
    "h_ori = origin_H_dict[PAIR_TYPE]\n",
    "h_trans = transformed_H_dict[PAIR_TYPE]\n",
    "h_rev = revert_H_dict[PAIR_TYPE]\n",
    "print(f\"Origin Hamiltonian Block (from hamiltonians.h5) Element Dtype: {h_ori.dtype} \\n\"\n",
    "      f\"Transformed Hamiltonian Block (from hamiltonians.h5) Element Dtype: {h_trans.dtype} \\n\", \n",
    "      f\"Reverted Hamiltonian Block (from hamiltonians.h5) Element Dtype: {h_rev.dtype} \\n\"\n",
    "      f\"Origin Hamiltonian Block (from hamiltonians.h5) Shape: {h_ori.shape} \\n\"\n",
    "      f\"Transformed Hamiltonian Block (from hamiltonians.h5) Shape: {h_trans.shape} \\n\", \n",
    "      f\"Reverted Hamiltonian Block (from hamiltonians.h5) Shape: {h_rev.shape} \\n\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\\n====== Test TensorProduct-DirectSum Block Transform ====== \")\n",
    "min_h_ori = h_ori.abs().min()\n",
    "min_h_rev = h_rev.abs().min()\n",
    "ave_h_ori = h_ori.abs().mean()\n",
    "ave_h_rev = h_rev.abs().mean()\n",
    "\n",
    "error = h_rev - h_ori\n",
    "max_error = error.abs().max()\n",
    "ave_error = error.abs().mean()\n",
    "is_correct: bool = bool((error == 0).all())\n",
    "print(f\"Is Test TensorProduct-DirectSum Block Transform Correct? {is_correct}.\")\n",
    "print(f\"Max Error: {max_error}.\\n\"\n",
    "      f\"Mean Error: {ave_error}.\\n\"\n",
    "      f\"(\"\n",
    "      f\"Min Absolutet of Origin Hamiltonian Block: {min_h_ori}\\n\"\n",
    "      f\"Min Absolute of Reverted Hamiltonian Block: {min_h_rev}\\n\"\n",
    "      f\"Mean Absolute Value Origin Hamiltonian Block: {ave_h_ori}\\n\"\n",
    "      f\"Mean Absolute Value of Reverted Hamiltonian Block: {ave_h_rev})\"\n",
    "      )\n",
    "rel_error = error / h_ori \n",
    "max_rel_error = rel_error.abs().max()\n",
    "print(f\"Max Relative Error: {max_rel_error:.3e}.\\n\")\n",
    "print(f\"Summary: Nearly Correct, L1Loss: {ave_error} hatree.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n====== Test revert_blocks_dict from graph.py ====== \")\n",
    "block_irreps_dict, simple_sorted_block_irreps_dict, sort_dict, inv_sort_dict = \\\n",
    "    read_block_irreps_from_one_OpenMX_folder(FOLDER, IS_SPIN)\n",
    "revert_H_dict = revert_blocks_dict(out_dict=transformed_H_dict, sort_dict=sort_dict,\n",
    "                                   Ls_dict=Ls_dict, is_spin=IS_SPIN, default_dtype_torch=DEFAULT_DTYPE_TORCH,\n",
    "                                   device='cpu')\n",
    "h_rev = revert_H_dict[PAIR_TYPE]\n",
    "min_h_rev = h_rev.abs().min()\n",
    "ave_h_rev = h_rev.abs().mean()\n",
    "error = h_rev - h_ori\n",
    "ave_error = error.abs().mean()\n",
    "print(f\"Just for test. Error is supposed to be large. L1Loss: {ave_error} hatree.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = To be Developed =\n",
    "\n",
    "# 2023/12/27 need infrastructure functions ! : \n",
    "#(Finished !) need Ls_dict in Dataset, \n",
    "#(Finished !) change into origin hamiltonian (real+imag part ? then we need to change the output of current func) in Dataset\n",
    "#(Finished !) change graph.txt into json format graph.json\n",
    "#(Finished !)  Test read directly from hamiltonians.h5 (Finished !) (need new func!) \n",
    "#       then get_label/x_dict -> revert_blocks_dict, outcome should not change\n",
    "#(Finished !) change Loss function in main_xxx.py (real+imag part)\n",
    "#(Notice !) print real+imag part loss seperately (only for analysis)\n",
    "#(Finished !) Test bug in Heterodataset with a very small dataset\n",
    "#(Notice !) Test totally read directly from hamiltonians.h5 then hetero(get_label/x_dict->sort) -> unsort or revert -> revert_blocks_dict, outcome should not change\n",
    "#(Notice !) blocks with designed shape have better to be wrapped in python class\n",
    "#(Notice !) Add Analyze Code for dataset, model, optimizer\n",
    "#       ---dataset data Average/Variance (List/Total)\n",
    "#       ---model parameter Number/Average/Variance (Module/Total, before train/train/after train)\n",
    "#       ---optimizer model parameter derivative Number/Average/Variance (Module/Total, before train/train/after train)\n",
    "#       ---Memory Usage (Module/Total)\n",
    "#(Finished !) Test Model Equivariance, Check! My Model Wrong or My Test Code Wrong ? Try with Simple and Correct new Net !\n",
    "#(Notice !) Many GPU Traning Support\n",
    "#(Notice !) Hamiltonian, Overlap Matrix Cutoff (distance)\n",
    "\n",
    "# 2023/12/28\n",
    "#(Finished !) (Notice !) The Data Type of edge identity varies:\n",
    "#       --- HeteroDataset (graph.py):  Tuple[str, str, str],   for example, ('83', '83-83', '83')\n",
    "#       --- Model (model.py):          str,                    for example, '83-83'    # torch.nn.ModuleDict should not contain Tuple as key, only str\n",
    "#       --- blocks_dict (graph.py):    Tuple[int, int],        for example, (83, 83)\n",
    "#       --- model.json (graph.py):     str,                    for example, '83-83'\n",
    "#(Finished !) Old Transform function (graph.py): \n",
    "#       'old' + ..\n",
    "#       --- str_to_tuple_pair_type, tuple_to_str_pair_type:   str <--> Tuple[int, int]\n",
    "#       --- data_to_tuple_pair_type, tuple_to_data_pair_type: Tuple[str, str, str] <--> Tuple[int, int]\n",
    "#       --- data_to_str_pair_type, str_to_data_pair_type:     str <--> Tuple[str, str, str]\n",
    "#(Finished !) Transform function (graph.py):\n",
    "#       --- to_edge\n",
    "\n",
    "# 2023/12/29 (1) refactor code\n",
    "#(Notice !) Enough Model Parameters: I should be able to test Equivariance of one single Module \n",
    "#(Notice !) choose which ? many variable <-> nn.Sequence \n",
    "#(Notice !) Train Epoch Time \"-21.46\" Bug\n",
    "#(Notice !) More Optimizer, LR_Schedular (how can I raise LR manually halfway when training ? )\n",
    "\n",
    "# 2023/12/29 (2) build model\n",
    "#(Notice ! Notice !) Why SOTA-Model-Equiformer reinitialize parameters like this ?\n",
    "#        self.rad = RadialProfile(fc_neurons + [self.dw.tp.weight_numel])\n",
    "#        for (slice, slice_sqrt_k) in self.dw.slices_sqrt_k.values():\n",
    "#           self.rad.net[-1].weight.data[slice, :] *= slice_sqrt_k\n",
    "#           self.rad.offset.data[slice] *= slice_sqrt_k\n",
    "#Uncorrect Answer: The target of self.rad(c) is to generate the weights for e3nn's TensorProduct.\n",
    "#       We should be consistent with e3nn's default weight initialization method: weight in Uniform[-1, +1].\n",
    "#       That is to say, the outcome of self.rad(RadialProfile) should be in Uniform[-1, +1].\n",
    "#       If we do not reinitialize, then\n",
    "#       self.rad: weight in Uniform[-1/sqrt(fan_in), +1/sqrt(fan_in)] (see torch.nn.Linear);\n",
    "#               offset in Uniform[-1/sqrt(fan_in), +1/sqrt(fan_in)] (see RadialProfile)\n",
    "#       Note that input of self.rad: edge_length_embedding in Uniform[-1, 1], then\n",
    "#               (Variace Formula) outcome of self.rad in Uniform[-1/sqrt(fan_in), +1/sqrt(fan_in)]\n",
    "#       If we do not reinitialize, then outcome's mean=0 correctly, \n",
    "#           but variance=1/sqrt(fan_in) not variance=1.\n",
    "#Why uncorrect: in SOTO model, if internal_weights=False, TensorProductRescale do not rescale.\n",
    "#Extra Note: e3nn's TensorProduct default weight initialization method: weight in Uniform[-1, +1], \n",
    "#           so If we generate weight manually(RadialProfile), we should keep in Uniform[-1, +1],\n",
    "#           but in Module using e3nn's TensorProduct, \n",
    "#           we appreciate the inital weight in Uniform[-1/sqrt(fan_in), +1/sqrt(fan_in)] like torch.nn.Linear\n",
    "#           and rescale the weight.\n",
    "#Better Method: A Module(self) contains TensorProduct, construct self.if_rescale\n",
    "#       ---TensorProduct internal_weights=True: if_rescale=True\n",
    "#       we need to rescale the auto-generated weight Uniform[-1, +1] -> Uniform[-1/sqrt(fan_in), +1/sqrt(fan_in)]\n",
    "#       ---TensorProduct internal_weights=False: if_rescale=False\n",
    "#       we keep our man-generated weight in Uniform[-1/sqrt(fan_in), +1/sqrt(fan_in)]. No need to rescale then.\n",
    "#(Finished !) Why SOTA-Model-Equiformer normalize one-hot node feature like this ?\n",
    "#        self.atom_type_lin = LinearRS(o3.Irreps('{}x0e'.format(self.max_atom_type)), \n",
    "#             self.irreps_node_embedding, bias=bias)\n",
    "#        self.atom_type_lin.tp.weight.data.mul_(self.max_atom_type ** 0.5)\n",
    "#Answer: node-atomic-number(num_nodes, ) -> one_hot(num_nodes, max_atom_type) --self.atom_type_lin--> node_fea\n",
    "#       I know Var(one_not)=1/max_atom_type and I appreciate Var(node_fea)=1.\n",
    "#(Notice !) Read source code:\n",
    "#       ---o3.spherical_harmonics:                  relative vector as input, edge_sh as output\n",
    "#       ---NodeEmbeddingNetwork(self.atom_embed):   atomic number one-hot dimension as input, atom_embedding as output\n",
    "#           |---LinearRS:                           tp with ones-vector (better Linear ?)\n",
    "#           |---TensorProductRescale:               tp with better init method\n",
    "#       ---GaussianRadialBasisLayer, RadialBasis(no access) (self.rbf):     relative distance as input, edge_length_embedding as input\n",
    "#       ---EdgeDegreeEmbeddingNetwork(self.edge_deg_embed): Convolution !   atom_embedding, edge_sh, edge_length_embedding and edge_index as input, edge_degree_embedding as output\n",
    "#           |---RadialProfile:  [Linear(fc_neuron)->LayerNorm->SiLU]->[Linear..]->+offset, notice bias of Linear before Norm is meaningless\n",
    "#(Notice !) Model HyperParameters:\n",
    "#       ---irreps_sh: relative vectors dimension increase\n",
    "#       ---irreps_node_embedding: atomic number one-hot dimension increase\n",
    "#       ---number_of_basis, self.max_radius: Parameter for GaussianRadialBasisLayer\n",
    "#       ---irreps_edge_attr == irreps_sh\n",
    "#(Notice !): why SOTA scale convolution layer with average degree of the graph (_AVG_DEGREE, a global constant) ?\n",
    "#(Notice !): why SOTA use LinearRS instead of e3nn's Linear ?\n",
    "\n",
    "#(Notice !) I start building model SOTA-Model-Equiformer by graph_attention_transformer_nonlinear_l2\n",
    "#        There are many other options actually in equiformer-master/nets/graph_attention_transformer.py.\n",
    "#(Notice ! Notice !) Add Analyzer Module: check Model medium variable Average and Variance, \n",
    "#          then judge initial method of model module (Linear, tp) parameters\n",
    "#          Use norm/intial-method in SOTO temporarily.\n",
    "#(Finished !) Use \"MLP\": lin(here)->gate(later)->Softmax-norm(later) instead of \"DotProduct\" for Score Matrix\n",
    "#(Finished !) Use \"MLP\": lin->gate->dtp->lin(no ->norm->gate after!) instead of \"No change\" for Value Matrix\n",
    "#(BUG !) graph.py, HeteroData's edge_index should use x_dict[str], x_dict[dst]'s index seperately,\n",
    "#               Not use global index (p ,q) ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeph_1107",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
