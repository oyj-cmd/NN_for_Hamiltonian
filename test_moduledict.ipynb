{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Part One\n",
    "1. `LinearModuleDict`\n",
    "2. `SelfGateModuleDict`\n",
    "3. `MultiLayerPerceptionModuleDict`\n",
    "4. `DepthwiseTensorProductModuleDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muyj/Download/default/anaconda3/envs/deeph_1107/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/muyj/Download/default/anaconda3/envs/deeph_1107/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/muyj/Download/default/anaconda3/envs/deeph_1107/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/muyj/Download/default/anaconda3/envs/deeph_1107/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/home/muyj/Download/default/anaconda3/envs/deeph_1107/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DepthwiseTensorProductModuleDict(\n",
      "  (dtp_dict): ModuleDict(\n",
      "    (DepthwiseTensorProductModule(6-8)): DepthwiseTensorProductModule(\n",
      "      (tp): TensorProductRescale(\n",
      "        (tp): TensorProduct(32x0e+16x1e x 8x0e+4x1e+4x2e+4x3e -> 32x0e+16x1e | 512 paths | 512 weights)\n",
      "        (bias): ParameterList()\n",
      "      )\n",
      "    )\n",
      "    (DepthwiseTensorProductModule(8-6)): DepthwiseTensorProductModule(\n",
      "      (tp): TensorProductRescale(\n",
      "        (tp): TensorProduct(8x0e+16x1e x 8x0e+4x1e+4x2e+4x3e -> 8x0e+16x1e | 320 paths | 320 weights)\n",
      "        (bias): ParameterList()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_dict): MultiLayerPerceptionModuleDict(\n",
      "    (mlp_dict): ModuleDict(\n",
      "      (MultiLayerPerceptionModule(6-8)): MultiLayerPerceptionModule(\n",
      "        (net): Sequential(\n",
      "          (0): LinearRS(\n",
      "            (tp): TensorProduct(16x0e x 1x0e -> 10x0e | 160 paths | 160 weights)\n",
      "            (bias): ParameterList()\n",
      "          )\n",
      "          (1): EquivariantLayerNormV2(10x0e, eps=1e-05)\n",
      "          (2): SelfGateModule(\n",
      "            (scalar_gate): Activation [x] (10x0e -> 10x0e)\n",
      "          )\n",
      "          (3): LinearRS(\n",
      "            (tp): TensorProduct(10x0e x 1x0e -> 512x0e | 5120 paths | 5120 weights)\n",
      "            (bias): ParameterList()\n",
      "          )\n",
      "          (4): EquivariantLayerNormV2(512x0e, eps=1e-05)\n",
      "          (5): SelfGateModule(\n",
      "            (scalar_gate): Activation [x] (512x0e -> 512x0e)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (MultiLayerPerceptionModule(8-6)): MultiLayerPerceptionModule(\n",
      "        (net): Sequential(\n",
      "          (0): LinearRS(\n",
      "            (tp): TensorProduct(16x0e x 1x0e -> 10x0e | 160 paths | 160 weights)\n",
      "            (bias): ParameterList()\n",
      "          )\n",
      "          (1): EquivariantLayerNormV2(10x0e, eps=1e-05)\n",
      "          (2): SelfGateModule(\n",
      "            (scalar_gate): Activation [x] (10x0e -> 10x0e)\n",
      "          )\n",
      "          (3): LinearRS(\n",
      "            (tp): TensorProduct(10x0e x 1x0e -> 320x0e | 3200 paths | 3200 weights)\n",
      "            (bias): ParameterList()\n",
      "          )\n",
      "          (4): EquivariantLayerNormV2(320x0e, eps=1e-05)\n",
      "          (5): SelfGateModule(\n",
      "            (scalar_gate): Activation [x] (320x0e -> 320x0e)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Mean Value of Output: 0.0032849046401679516\n",
      "Variance value of Output: 6.625950813293457\n",
      "(Mean Value of Input: -0.02098892070353031\n",
      "Variance value of Input: 0.9495509266853333)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import e3nn\n",
    "from e3nn.o3 import Irreps\n",
    "\n",
    "# edge_a, edge_b: \"6-8\", \"8-6\"\n",
    "irreps_edges_a: Irreps = Irreps(\"32x0e+16x1e\")\n",
    "irreps_edges_b: Irreps = Irreps(\"8x0e+16x1e\")\n",
    "irreps_edge_dict: Dict[str, Irreps] = {\"6-8\": irreps_edges_a, \"8-6\": irreps_edges_b}\n",
    "irreps_edge_length: Irreps = Irreps(\"16x0e\") # should only contain Irrep=\"0e\"\n",
    "irreps_edge_vec: Irreps = Irreps(\"8x0e+4x1e+4x2e+4x3e\")\n",
    "\n",
    "num_edges_a: int = 7\n",
    "num_edges_b: int = 12\n",
    "num_heads: int = 3\n",
    "\n",
    "\n",
    "# = parameter 0 = \n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "edge_fea_a = irreps_edges_a.randn(num_edges_a, -1, normalization=\"component\")\n",
    "edge_fea_b = irreps_edges_b.randn(num_edges_b, -1, normalization=\"component\")\n",
    "edge_fea_a = irreps_edges_a.randn(num_edges_a, num_heads, -1, normalization=\"component\")\n",
    "edge_fea_b = irreps_edges_b.randn(num_edges_b, num_heads, -1, normalization=\"component\")\n",
    "edge_fea_dict = {\"6-8\": edge_fea_a, \"8-6\": edge_fea_b}\n",
    "edge_vec_a = irreps_edge_vec.randn(num_edges_a, -1, normalization=\"component\")\n",
    "edge_vec_b = torch.randn(1) * irreps_edge_vec.randn(num_edges_b, -1, normalization=\"component\") \n",
    "edge_vec_dict = {\"6-8\": edge_vec_a, \"8-6\": edge_vec_b}\n",
    "edge_length_dict = {\"6-8\": irreps_edge_length.randn(num_edges_a, -1, normalization=\"component\"), \n",
    "                    \"8-6\": irreps_edge_length.randn(num_edges_b, -1, normalization=\"component\")}\n",
    "\n",
    "from modelname.nn.moduledict import LinearModuleDict\n",
    "from modelname.nn.moduledict import SelfGateModuleDict\n",
    "from modelname.nn.moduledict import MultiLayerPerceptionModuleDict\n",
    "from modelname.nn.moduledict import DepthwiseTensorProductModuleDict\n",
    "net_1 = LinearModuleDict(irreps_in_dict=irreps_edge_dict, irreps_out_dict=irreps_edge_dict, bias=True)\n",
    "net_2 = SelfGateModuleDict(irreps_in_dict=irreps_edge_dict, act_scalars=torch.nn.SiLU(), act_gates=torch.nn.Sigmoid())\n",
    "net_3 = MultiLayerPerceptionModuleDict(irreps_in_dict=irreps_edge_dict, irreps_out_dict=irreps_edge_dict,\n",
    "        irreps_mid_list=[Irreps(\"4x0e+2x1e+1x2e\")], add_last_linear=\"bias\", if_act=True, if_norm=True, \n",
    "        act_scalars=torch.nn.SiLU(), act_gates=torch.nn.Sigmoid(), norm_type=\"layer\"\n",
    "    )   \n",
    "#TODO Why get wrong out variance when irreps_mid_list contain \"1e\" ?\n",
    "#TODO Why rescale=True/False, \"1e\" Variance always 10 times than \"0e\" ?\n",
    "net_4 = DepthwiseTensorProductModuleDict(irreps_in_dict=irreps_edge_dict, \n",
    "        irreps_edge_vec_embed=irreps_edge_vec, \n",
    "        irreps_edge_length_embed=irreps_edge_length,\n",
    "        dtp_internal_weights=False,\n",
    "        mlp_irreps_mid_list=[Irreps(\"10x0e\")], # should only contain Irrep=\"0e\"\n",
    "        mlp_add_last_linear=None, \n",
    "        mlp_if_act=True,\n",
    "        mlp_if_norm=True,\n",
    "        mlp_act_scalars=torch.nn.SiLU(), mlp_act_gates=torch.nn.Sigmoid(),\n",
    "        mlp_norm_type=\"layer\"\n",
    "    )   # dtp_internal_weights=False for alpha\n",
    "        # mlp_add_last_linear=\"bias\" for alpha\n",
    "        # mlp_if_act=True for alpha\n",
    "        # mlp_if_norm=True for alpha\n",
    "        # mlp_act_scalars=torch.nn.SiLU(), mlp_act_gates=torch.nn.Sigmoid() for alpha\n",
    "        # mlp_norm_type=\"layer\" for alpha\n",
    "        # dtp_internal_weights=True for value\n",
    "#TODO mlp_add_last_linear=None, mlp_if_norm=True seems get better Out Variace\n",
    "\n",
    "# = parameter 1 = \n",
    "net = net_4 # net_1 or net_2 or net_3 or net_4 \n",
    "\n",
    "print(net) # Note that non-scalars have no bias even bias=True\n",
    "if isinstance(net, DepthwiseTensorProductModuleDict):\n",
    "    out = net(edge_fea_dict=edge_fea_dict, edge_vec_dict=edge_vec_dict, edge_length_dict=edge_length_dict)\n",
    "else:   \n",
    "    out = net(x_dict=edge_fea_dict)\n",
    "\n",
    "# = parameter 2 = \n",
    "key = \"8-6\" # \"8-6\" or \"6-8\"\n",
    "in_ = edge_fea_dict[key]\n",
    "in_var = in_.var()\n",
    "in_mean = in_.mean()\n",
    "out_ = out[key]\n",
    "out_var = out_.var()\n",
    "out_mean = out_.mean()\n",
    "print(\n",
    "    f\"Mean Value of Output: {out_mean}\\n\"\n",
    "    f\"Variance value of Output: {out_var}\\n\"\n",
    "    f\"(\"\n",
    "    f\"Mean Value of Input: {in_mean}\\n\"\n",
    "    f\"Variance value of Input: {in_var})\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1334,  0.0134,  0.9910],\n",
      "        [-0.5643, -0.8230, -0.0649],\n",
      "        [ 0.8147, -0.5678,  0.1174]])\n",
      "Is DepthwiseTensorProductModuleDict rotation equivariant?\n",
      "Max Error: 1.5735626220703125e-05.\n",
      "Mean Error (L1Loss): 1.2073162451997632e-06.\n",
      "(Min Absolute of out_1: 0.00013166152348276228\n",
      "Min Absolute of out_2: 0.00013256072998046875\n",
      "Max Absolute of out_1: 18.542585372924805\n",
      "Max Absolute of out_2: 18.54258918762207\n",
      "Mean Absolute Value out_1: 1.7347888946533203\n",
      "Mean Absolute Value of out_2: 1.7347891330718994)\n"
     ]
    }
   ],
   "source": [
    "def outs_rot_invariance(rot: Tensor, \n",
    "                  model: torch.nn.Module, \n",
    "                  x_dict: Dict[str, Tensor],\n",
    "                  irreps_in_dict: Dict[str, Irreps],\n",
    "                  irreps_out_dict: Dict[str, Irreps]=None, # not used in DepthwiseTensorProductModuleDict\n",
    "                  irreps_edge_vec: Irreps=None, \n",
    "                  irreps_edge_length: Irreps=None,\n",
    "                  edge_vec_dict: Dict[str, Tensor]=None, \n",
    "                  edge_length_dict: Dict[str, Tensor]=None):\n",
    "    \"\"\"Unit test for checking whether a model (GNN model/layer) is \n",
    "    rotation equivariant.\n",
    "    \"\"\"\n",
    "    # Rotate Output\n",
    "    if isinstance(model, DepthwiseTensorProductModuleDict):\n",
    "        out_1: Dict[str, Tensor] = model(edge_fea_dict=x_dict, edge_vec_dict=edge_vec_dict, edge_length_dict=edge_length_dict)\n",
    "        irreps_out_dict = irreps_in_dict\n",
    "    else:\n",
    "        out_1: Dict[str, Tensor] = model(x_dict=x_dict)\n",
    "    for edge, v in out_1.items():\n",
    "        irreps_out: Irreps = irreps_out_dict[edge]\n",
    "        D_out = irreps_out.D_from_matrix(rot)\n",
    "        out_1[edge] = v @ D_out.T\n",
    "\n",
    "    # Rotate Input\n",
    "    new_x_dict: Dict[str, Tensor] = {}\n",
    "    for edge, v in x_dict.items():\n",
    "        irreps_in: Irreps = irreps_in_dict[edge]\n",
    "        D_in = irreps_in.D_from_matrix(rot)\n",
    "        #TODO (Notice !) data is not changed here ! WHY ?\n",
    "        # x_dict[edge] = v @ D_in.T         ## (Notice !)\n",
    "        new_x_dict[edge] = v @ D_in.T       ## (Notice !)\n",
    "\n",
    "    if isinstance(model, DepthwiseTensorProductModuleDict):\n",
    "        new_edge_vec_dict: Dict[str, Tensor] = {}\n",
    "        new_edge_length_dict: Dict[str, Tensor] = {}\n",
    "        for edge, _ in x_dict.items():\n",
    "            irreps_in_vec: Irreps = irreps_edge_vec\n",
    "            irreps_in_length: Irreps = irreps_edge_length\n",
    "            D_in_vec = irreps_in_vec.D_from_matrix(rot)\n",
    "            D_in_length = irreps_in_length.D_from_matrix(rot)\n",
    "            new_edge_vec_dict[edge] = edge_vec_dict[edge] @ D_in_vec.T       \n",
    "            new_edge_length_dict[edge] = edge_length_dict[edge] @ D_in_length.T\n",
    "\n",
    "    # Forward pass on rotated example\n",
    "    if isinstance(model, DepthwiseTensorProductModuleDict):\n",
    "        out_2: Dict[str, Tensor] = model(edge_fea_dict=new_x_dict, \n",
    "                edge_vec_dict=new_edge_vec_dict, edge_length_dict=new_edge_length_dict)\n",
    "    else:\n",
    "        out_2 = model(x_dict=new_x_dict)\n",
    "    \n",
    "    return out_1, out_2\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "rot = e3nn.o3.rand_matrix()\n",
    "print(rot)\n",
    "\n",
    "out_1_dict, out_2_dict = outs_rot_invariance(\n",
    "    rot=rot, model=net, x_dict=edge_fea_dict,\n",
    "    irreps_in_dict=irreps_edge_dict, irreps_out_dict=irreps_edge_dict,\n",
    "\n",
    "    irreps_edge_vec=irreps_edge_vec, \n",
    "    irreps_edge_length=irreps_edge_length,\n",
    "    edge_vec_dict=edge_vec_dict, \n",
    "    edge_length_dict=edge_length_dict\n",
    ")\n",
    "\n",
    "# = parameter 3 = \n",
    "out_1 = out_1_dict[key]\n",
    "out_2 = out_2_dict[key]\n",
    "min_out_1 = out_1.abs().min()\n",
    "min_out_2 = out_2.abs().min()\n",
    "max_out_1 = out_1.abs().max()\n",
    "max_out_2 = out_2.abs().max()\n",
    "ave_out_1 = out_1.abs().mean()\n",
    "ave_out_2 = out_2.abs().mean()\n",
    "\n",
    "error = out_2 - out_1\n",
    "max_error = error.abs().max()\n",
    "ave_error = error.abs().mean()\n",
    "\n",
    "print(f\"Is {type(net).__name__} rotation equivariant?\")\n",
    "print(f\"Max Error: {max_error}.\\n\"\n",
    "      f\"Mean Error (L1Loss): {ave_error}.\\n\"\n",
    "      f\"(\"\n",
    "      f\"Min Absolute of out_1: {min_out_1}\\n\"\n",
    "      f\"Min Absolute of out_2: {min_out_2}\\n\"\n",
    "      f\"Max Absolute of out_1: {max_out_1}\\n\"\n",
    "      f\"Max Absolute of out_2: {max_out_2}\\n\"\n",
    "      f\"Mean Absolute Value out_1: {ave_out_1}\\n\"\n",
    "      f\"Mean Absolute Value of out_2: {ave_out_2})\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Part Two\n",
    "5. `ElementLevelScatterModuleDict`\n",
    "6. `TypeLevelScatterModuleDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeLevelScatterModuleDict()\n",
      "torch.Size([6, 3, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muyj/Project/Project_1106_deephe3_example/modelname/nn/moduledict.py:454: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  num_type_scatter = out_scatter.new_tensor(num_type_scatter)\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import e3nn\n",
    "from e3nn.o3 import Irreps\n",
    "\n",
    "# edge_a, edge_b: \"6-8\", \"8-6\"\n",
    "# node_a, node_b: \"6\", \"8\"\n",
    "\n",
    "irreps_nodes_a: Irreps = Irreps(\"16x0e+8x1e+4x2e\")\n",
    "irreps_nodes_b: Irreps = Irreps(\"16x0e+8x1e+4x2e\")\n",
    "irreps_node_dict: Dict[str, Irreps] = {\"6\": irreps_nodes_a, \"8\": irreps_nodes_b}\n",
    "irreps_edges_a: Irreps = Irreps(\"1x0e+16x2e\")\n",
    "irreps_edges_b: Irreps = Irreps(\"8x0e+16x1e\")\n",
    "irreps_edge_dict: Dict[str, Irreps] = {\"6-8\": irreps_edges_a, \"8-6\": irreps_edges_b}\n",
    "\n",
    "num_nodes_a: int = 6\n",
    "num_nodes_b: int = 12\n",
    "num_nodes_dict: Dict[str, int] = {\"6\": num_nodes_a, \"8\": num_nodes_b}\n",
    "edge_index_a: Tensor = torch.tensor([[x, y] for x in range(num_nodes_a) for y in range(num_nodes_b-1)]).T.contiguous()\n",
    "edge_index_b: Tensor = torch.tensor([[y, x] for x in range(num_nodes_a-1) for y in range(num_nodes_b-1)]).T.contiguous()\n",
    "# nearly fully connected, last \"6\" is isolated in \"8-6\", last \"8\" is isolated in both.\n",
    "edge_index_dict = {\"6-8\": edge_index_a, \"8-6\": edge_index_b} \n",
    "\n",
    "num_edges_a: int = edge_index_a.shape[1]\n",
    "num_edges_b: int = edge_index_b.shape[1]\n",
    "num_heads: int = 3\n",
    "\n",
    "# = parameter 0 = \n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "edge_fea_a = irreps_edges_a.randn(num_edges_a, -1, normalization=\"component\")\n",
    "edge_fea_b = irreps_edges_b.randn(num_edges_b, -1, normalization=\"component\")\n",
    "edge_fea_a = irreps_edges_a.randn(num_edges_a, num_heads, -1, normalization=\"component\")\n",
    "edge_fea_b = irreps_edges_b.randn(num_edges_b, num_heads, -1, normalization=\"component\")\n",
    "edge_fea_dict = {\"6-8\": edge_fea_a, \"8-6\": edge_fea_b}\n",
    "\n",
    "from modelname.nn.moduledict import ElementLevelScatterModuleDict\n",
    "from modelname.nn.moduledict import TypeLevelScatterModuleDict\n",
    "net_5 = ElementLevelScatterModuleDict(irreps_message_dict=irreps_edge_dict, irreps_node_dict=irreps_node_dict)\n",
    "net_6 = TypeLevelScatterModuleDict()\n",
    "\n",
    "# = parameter 1 = \n",
    "net = net_6 # net_5 or net_6\n",
    "print(net) # Note that non-scalars have no bias even bias=True\n",
    "\n",
    "if isinstance(net, ElementLevelScatterModuleDict):\n",
    "    out, other = net(message_dict=edge_fea_dict, edge_index_dict=edge_index_dict, num_nodes_dict=num_nodes_dict)\n",
    "elif isinstance(net, TypeLevelScatterModuleDict):\n",
    "    out, other = net_5(message_dict=edge_fea_dict, edge_index_dict=edge_index_dict, num_nodes_dict=num_nodes_dict)\n",
    "    out = net(type_message_dict=out, num_types_dict=other)\n",
    "\n",
    "# = parameter 2 =\n",
    "key: str = \"6\" # \"6\" or \"8\" \n",
    "print(out[key].shape)\n",
    "if isinstance(net, ElementLevelScatterModuleDict):\n",
    "    print(other[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1334,  0.0134,  0.9910],\n",
      "        [-0.5643, -0.8230, -0.0649],\n",
      "        [ 0.8147, -0.5678,  0.1174]])\n",
      "Is TypeLevelScatterModuleDict rotation equivariant?\n",
      "Max Error: 1.7881393432617188e-07.\n",
      "Mean Error (L1Loss): 1.2281034855732287e-08.\n",
      "(Min Absolute of out_1: 0.0\n",
      "Min Absolute of out_2: 0.0\n",
      "Max Absolute of out_1: 1.086525321006775\n",
      "Max Absolute of out_2: 1.0865254402160645\n",
      "Mean Absolute Value out_1: 0.13905927538871765\n",
      "Mean Absolute Value of out_2: 0.13905927538871765)\n"
     ]
    }
   ],
   "source": [
    "def outs_rot_invariance(rot: Tensor, \n",
    "                  model: torch.nn.Module, \n",
    "\n",
    "                  irreps_in_dict: Dict[str, Irreps], \n",
    "                  irreps_out_dict: Dict[str, Irreps],\n",
    "\n",
    "                  edge_fea_dict: Dict[str, Tensor],\n",
    "                  edge_index_dict: Dict[str, Tensor],\n",
    "                  num_nodes_dict: Dict[str, int],\n",
    "                  ):\n",
    "    \"\"\"Unit test for checking whether a model (GNN model/layer) is \n",
    "    rotation equivariant.\n",
    "    \"\"\"\n",
    "    # Rotate Output\n",
    "    if isinstance(model, ElementLevelScatterModuleDict):\n",
    "        out_1, others_1 = model(message_dict=edge_fea_dict, edge_index_dict=edge_index_dict, num_nodes_dict=num_nodes_dict)\n",
    "    elif isinstance(model, TypeLevelScatterModuleDict):\n",
    "        out_1, others_1 = net_5(message_dict=edge_fea_dict, edge_index_dict=edge_index_dict, num_nodes_dict=num_nodes_dict)\n",
    "        out_1 = model(type_message_dict=out_1, num_types_dict=others_1)\n",
    "\n",
    "    for edge, v in out_1.items():\n",
    "        irreps_out: Irreps = irreps_out_dict[edge]\n",
    "        D_out = irreps_out.D_from_matrix(rot)\n",
    "        out_1[edge] = v @ D_out.T\n",
    "\n",
    "    # Rotate Input\n",
    "    new_edge_fea_dict: Dict[str, Tensor] = {}\n",
    "    for edge, v in edge_fea_dict.items():\n",
    "        irreps_in: Irreps = irreps_in_dict[edge]\n",
    "        D_in = irreps_in.D_from_matrix(rot)\n",
    "        #TODO (Notice !) data is not changed here ! WHY ?\n",
    "        # x_dict[edge] = v @ D_in.T         ## (Notice !)\n",
    "        new_edge_fea_dict[edge] = v @ D_in.T       ## (Notice !)\n",
    "\n",
    "    # Forward pass on rotated example\n",
    "    if isinstance(model, ElementLevelScatterModuleDict):\n",
    "        out_2, others_2 = model(message_dict=new_edge_fea_dict, edge_index_dict=edge_index_dict, num_nodes_dict=num_nodes_dict)\n",
    "    elif isinstance(model, TypeLevelScatterModuleDict):\n",
    "        out_2, others_2 = net_5(message_dict=new_edge_fea_dict, edge_index_dict=edge_index_dict, num_nodes_dict=num_nodes_dict)\n",
    "        out_2 = model(type_message_dict=out_2, num_types_dict=others_2)\n",
    "\n",
    "    return out_1, out_2\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "rot = e3nn.o3.rand_matrix()\n",
    "print(rot)\n",
    "\n",
    "out_1_dict, out_2_dict = outs_rot_invariance(\n",
    "    rot=rot, model=net, \n",
    "    irreps_in_dict=irreps_edge_dict, irreps_out_dict=irreps_node_dict,\n",
    "\n",
    "    edge_fea_dict=edge_fea_dict,\n",
    "    edge_index_dict=edge_index_dict,\n",
    "    num_nodes_dict=num_nodes_dict,\n",
    "    \n",
    ")\n",
    "\n",
    "# = parameter 3 = \n",
    "out_1 = out_1_dict[key]\n",
    "out_2 = out_2_dict[key]\n",
    "min_out_1 = out_1.abs().min()\n",
    "min_out_2 = out_2.abs().min()\n",
    "max_out_1 = out_1.abs().max()\n",
    "max_out_2 = out_2.abs().max()\n",
    "ave_out_1 = out_1.abs().mean()\n",
    "ave_out_2 = out_2.abs().mean()\n",
    "\n",
    "error = out_2 - out_1\n",
    "max_error = error.abs().max()\n",
    "ave_error = error.abs().mean()\n",
    "\n",
    "print(f\"Is {type(net).__name__} rotation equivariant?\")\n",
    "print(f\"Max Error: {max_error}.\\n\"\n",
    "      f\"Mean Error (L1Loss): {ave_error}.\\n\"\n",
    "      f\"(\"\n",
    "      f\"Min Absolute of out_1: {min_out_1}\\n\"\n",
    "      f\"Min Absolute of out_2: {min_out_2}\\n\"\n",
    "      f\"Max Absolute of out_1: {max_out_1}\\n\"\n",
    "      f\"Max Absolute of out_2: {max_out_2}\\n\"\n",
    "      f\"Mean Absolute Value out_1: {ave_out_1}\\n\"\n",
    "      f\"Mean Absolute Value of out_2: {ave_out_2})\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Part Three\n",
    "7. `SoftmaxScatteNormModuleDict` (Only for \"0e\" Scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftmaxScatterNormModuleDict()\n",
      "Mean Value of Output: 0.09090909361839294\n",
      "Variance value of Output: 0.007198275998234749\n",
      "(Mean Value of Input: 0.023028507828712463\n",
      "Variance value of Input: 0.9667189717292786)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import e3nn\n",
    "from e3nn.o3 import Irreps\n",
    "\n",
    "# edge_a, edge_b: \"6-8\", \"8-6\"\n",
    "irreps_edges_a: Irreps = Irreps(\"10x0e\")\n",
    "irreps_edges_b: Irreps = Irreps(\"8x0e\")\n",
    "irreps_edge_dict: Dict[str, Irreps] = {\"6-8\": irreps_edges_a, \"8-6\": irreps_edges_b}\n",
    "\n",
    "num_nodes_a: int = 6\n",
    "num_nodes_b: int = 12\n",
    "num_nodes_dict: Dict[str, int] = {\"6\": num_nodes_a, \"8\": num_nodes_b}\n",
    "edge_index_a: Tensor = torch.tensor([[x, y] for x in range(num_nodes_a) for y in range(num_nodes_b-1)]).T.contiguous()\n",
    "edge_index_b: Tensor = torch.tensor([[y, x] for x in range(num_nodes_a-1) for y in range(num_nodes_b-1)]).T.contiguous()\n",
    "# nearly fully connected, last \"6\" is isolated in \"8-6\", last \"8\" is isolated in both.\n",
    "edge_index_dict = {\"6-8\": edge_index_a, \"8-6\": edge_index_b} \n",
    "\n",
    "num_edges_a: int = edge_index_a.shape[1]\n",
    "num_edges_b: int = edge_index_b.shape[1]\n",
    "num_heads: int = 3\n",
    "\n",
    "\n",
    "# = parameter 0 = \n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "edge_fea_a = irreps_edges_a.randn(num_edges_a, -1, normalization=\"component\")\n",
    "edge_fea_b = irreps_edges_b.randn(num_edges_b, -1, normalization=\"component\")\n",
    "edge_fea_a = irreps_edges_a.randn(num_edges_a, num_heads, -1, normalization=\"component\")\n",
    "edge_fea_b = irreps_edges_b.randn(num_edges_b, num_heads, -1, normalization=\"component\")\n",
    "edge_fea_dict = {\"6-8\": edge_fea_a, \"8-6\": edge_fea_b}\n",
    "\n",
    "\n",
    "from modelname.nn.moduledict import SoftmaxScatterNormModuleDict\n",
    "net_7 = SoftmaxScatterNormModuleDict(irreps_in_dict=irreps_edge_dict)\n",
    "\n",
    "# = parameter 1 = \n",
    "net = net_7 # net_7\n",
    "print(net) # Note that non-scalars have no bias even bias=True\n",
    "if isinstance(net, SoftmaxScatterNormModuleDict):\n",
    "    out = net(edge_fea_dict=edge_fea_dict , edge_index_dict=edge_index_dict, num_nodes_dict=num_nodes_dict)\n",
    "\n",
    "# = parameter 2 = \n",
    "key = \"8-6\" # \"8-6\" or \"6-8\"\n",
    "in_ = edge_fea_dict[key]\n",
    "in_var = in_.var()\n",
    "in_mean = in_.mean()\n",
    "out_ = out[key]\n",
    "out_var = out_.var()\n",
    "out_mean = out_.mean()\n",
    "print(\n",
    "    f\"Mean Value of Output: {out_mean}\\n\"\n",
    "    f\"Variance value of Output: {out_var}\\n\"\n",
    "    f\"(\"\n",
    "    f\"Mean Value of Input: {in_mean}\\n\"\n",
    "    f\"Variance value of Input: {in_var})\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Part Four\n",
    "7. `TransformerModuleDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModuleDict(\n",
      "  (node_src_linear_dict): LinearModuleDict(\n",
      "    (linear_dict): ModuleDict(\n",
      "      (Linear(6-8)): LinearRS(\n",
      "        (tp): TensorProduct(16x0e+8x1e+4x2e x 1x0e -> 32x0e+16x1e | 640 paths | 640 weights)\n",
      "        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 32])\n",
      "      )\n",
      "      (Linear(8-6)): LinearRS(\n",
      "        (tp): TensorProduct(16x0e+8x1e+4x2e x 1x0e -> 8x0e+16x1e | 256 paths | 256 weights)\n",
      "        (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 8])\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (node_dst_linear_dict): LinearModuleDict(\n",
      "    (linear_dict): ModuleDict(\n",
      "      (Linear(6-8)): LinearRS(\n",
      "        (tp): TensorProduct(16x0e+8x1e+4x2e x 1x0e -> 32x0e+16x1e | 640 paths | 640 weights)\n",
      "        (bias): ParameterList()\n",
      "      )\n",
      "      (Linear(8-6)): LinearRS(\n",
      "        (tp): TensorProduct(16x0e+8x1e+4x2e x 1x0e -> 8x0e+16x1e | 256 paths | 256 weights)\n",
      "        (bias): ParameterList()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (message_dtp_dict): DepthwiseTensorProductModuleDict(\n",
      "    (dtp_dict): ModuleDict(\n",
      "      (DepthwiseTensorProductModule(6-8)): DepthwiseTensorProductModule(\n",
      "        (tp): TensorProductRescale(\n",
      "          (tp): TensorProduct(32x0e+16x1e x 8x0e+4x1e+4x2e+4x3e -> 32x0e+16x1e | 512 paths | 512 weights)\n",
      "          (bias): ParameterList()\n",
      "        )\n",
      "      )\n",
      "      (DepthwiseTensorProductModule(8-6)): DepthwiseTensorProductModule(\n",
      "        (tp): TensorProductRescale(\n",
      "          (tp): TensorProduct(8x0e+16x1e x 8x0e+4x1e+4x2e+4x3e -> 8x0e+16x1e | 320 paths | 320 weights)\n",
      "          (bias): ParameterList()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mlp_dict): MultiLayerPerceptionModuleDict(\n",
      "      (mlp_dict): ModuleDict(\n",
      "        (MultiLayerPerceptionModule(6-8)): MultiLayerPerceptionModule(\n",
      "          (net): Sequential(\n",
      "            (0): LinearRS(\n",
      "              (tp): TensorProduct(16x0e x 1x0e -> 64x0e | 1024 paths | 1024 weights)\n",
      "              (bias): ParameterList()\n",
      "            )\n",
      "            (1): EquivariantLayerNormV2(64x0e, eps=1e-05)\n",
      "            (2): SelfGateModule(\n",
      "              (scalar_gate): Activation [x] (64x0e -> 64x0e)\n",
      "            )\n",
      "            (3): LinearRS(\n",
      "              (tp): TensorProduct(64x0e x 1x0e -> 64x0e | 4096 paths | 4096 weights)\n",
      "              (bias): ParameterList()\n",
      "            )\n",
      "            (4): EquivariantLayerNormV2(64x0e, eps=1e-05)\n",
      "            (5): SelfGateModule(\n",
      "              (scalar_gate): Activation [x] (64x0e -> 64x0e)\n",
      "            )\n",
      "            (6): LinearRS(\n",
      "              (tp): TensorProduct(64x0e x 1x0e -> 512x0e | 32768 paths | 32768 weights)\n",
      "              (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 512])\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (MultiLayerPerceptionModule(8-6)): MultiLayerPerceptionModule(\n",
      "          (net): Sequential(\n",
      "            (0): LinearRS(\n",
      "              (tp): TensorProduct(16x0e x 1x0e -> 64x0e | 1024 paths | 1024 weights)\n",
      "              (bias): ParameterList()\n",
      "            )\n",
      "            (1): EquivariantLayerNormV2(64x0e, eps=1e-05)\n",
      "            (2): SelfGateModule(\n",
      "              (scalar_gate): Activation [x] (64x0e -> 64x0e)\n",
      "            )\n",
      "            (3): LinearRS(\n",
      "              (tp): TensorProduct(64x0e x 1x0e -> 64x0e | 4096 paths | 4096 weights)\n",
      "              (bias): ParameterList()\n",
      "            )\n",
      "            (4): EquivariantLayerNormV2(64x0e, eps=1e-05)\n",
      "            (5): SelfGateModule(\n",
      "              (scalar_gate): Activation [x] (64x0e -> 64x0e)\n",
      "            )\n",
      "            (6): LinearRS(\n",
      "              (tp): TensorProduct(64x0e x 1x0e -> 320x0e | 20480 paths | 20480 weights)\n",
      "              (bias): ParameterList(  (0): Parameter containing: [torch.float32 of size 320])\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (message_vec2heads_dict): Vec2AttnHeadsModuleDict(\n",
      "    (vec2heads_dict): ModuleDict(\n",
      "      (Vec2AttnHeads(6-8)): Vec2AttnHeads()\n",
      "      (Vec2AttnHeads(8-6)): Vec2AttnHeads()\n",
      "    )\n",
      "  )\n",
      "  (head_message_mlp_to_alpha_dict): MultiLayerPerceptionModuleDict(\n",
      "    (mlp_dict): ModuleDict(\n",
      "      (MultiLayerPerceptionModule(6-8)): MultiLayerPerceptionModule(\n",
      "        (net): Sequential(\n",
      "          (0): LinearRS(\n",
      "            (tp): TensorProduct(8x0e+4x1e x 1x0e -> 8x0e | 64 paths | 64 weights)\n",
      "            (bias): ParameterList()\n",
      "          )\n",
      "          (1): EquivariantLayerNormV2(8x0e, eps=1e-05)\n",
      "          (2): SelfGateModule(\n",
      "            (scalar_gate): Activation [x] (8x0e -> 8x0e)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (MultiLayerPerceptionModule(8-6)): MultiLayerPerceptionModule(\n",
      "        (net): Sequential(\n",
      "          (0): LinearRS(\n",
      "            (tp): TensorProduct(2x0e+4x1e x 1x0e -> 2x0e | 4 paths | 4 weights)\n",
      "            (bias): ParameterList()\n",
      "          )\n",
      "          (1): EquivariantLayerNormV2(2x0e, eps=1e-05)\n",
      "          (2): SelfGateModule(\n",
      "            (scalar_gate): Activation [x] (2x0e -> 2x0e)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head_alpha_linear): LinearModuleDict(\n",
      "    (linear_dict): ModuleDict(\n",
      "      (Linear(6-8)): LinearRS(\n",
      "        (tp): TensorProduct(8x0e x 1x0e -> 1x0e | 8 paths | 8 weights)\n",
      "        (bias): ParameterList()\n",
      "      )\n",
      "      (Linear(8-6)): LinearRS(\n",
      "        (tp): TensorProduct(2x0e x 1x0e -> 1x0e | 2 paths | 2 weights)\n",
      "        (bias): ParameterList()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head_alpha_softmaxnorm): SoftmaxScatterNormModuleDict()\n",
      "  (head_alpha_dropout): DropoutModuleDict(\n",
      "    (dropout_dict): ModuleDict(\n",
      "      (Dropout(6-8)): Dropout(p=0.0, inplace=False)\n",
      "      (Dropout(8-6)): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (head_message_mlp_to_value_dict): MultiLayerPerceptionModuleDict(\n",
      "    (mlp_dict): ModuleDict(\n",
      "      (MultiLayerPerceptionModule(6-8)): MultiLayerPerceptionModule(\n",
      "        (net): Sequential(\n",
      "          (0): LinearRS(\n",
      "            (tp): TensorProduct(8x0e+4x1e x 1x0e -> 8x0e+4x1e | 80 paths | 80 weights)\n",
      "            (bias): ParameterList()\n",
      "          )\n",
      "          (1): EquivariantLayerNormV2(8x0e+4x1e, eps=1e-05)\n",
      "          (2): SelfGateModule(\n",
      "            (dot_product_x1): TensorProduct(4x1e x 4x1e -> 4x0e | 4 paths | 0 weights)\n",
      "            (gate): Gate (12x0e+4x1e -> 8x0e+4x1e)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (MultiLayerPerceptionModule(8-6)): MultiLayerPerceptionModule(\n",
      "        (net): Sequential(\n",
      "          (0): LinearRS(\n",
      "            (tp): TensorProduct(2x0e+4x1e x 1x0e -> 2x0e+4x1e | 20 paths | 20 weights)\n",
      "            (bias): ParameterList()\n",
      "          )\n",
      "          (1): EquivariantLayerNormV2(2x0e+4x1e, eps=1e-05)\n",
      "          (2): SelfGateModule(\n",
      "            (dot_product_x1): TensorProduct(4x1e x 4x1e -> 4x0e | 4 paths | 0 weights)\n",
      "            (gate): Gate (6x0e+4x1e -> 2x0e+4x1e)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attn_heads2vec_dict): AttnHeads2VecModuleDict(\n",
      "    (vec2heads_dict): ModuleDict(\n",
      "      (AttnHeads2Vec(6-8)): AttnHeads2Vec(irreps_head=8x0e+4x1e)\n",
      "      (AttnHeads2Vec(8-6)): AttnHeads2Vec(irreps_head=2x0e+4x1e)\n",
      "    )\n",
      "  )\n",
      "  (attn_element_level_scatter_dict): ElementLevelScatterModuleDict(\n",
      "    (linear_dict): LinearModuleDict(\n",
      "      (linear_dict): ModuleDict(\n",
      "        (Linear(6-8)): LinearRS(\n",
      "          (tp): TensorProduct(32x0e+16x1e x 1x0e -> 16x0e+8x1e+4x2e | 640 paths | 640 weights)\n",
      "          (bias): ParameterList()\n",
      "        )\n",
      "        (Linear(8-6)): LinearRS(\n",
      "          (tp): TensorProduct(8x0e+16x1e x 1x0e -> 16x0e+8x1e+4x2e | 256 paths | 256 weights)\n",
      "          (bias): ParameterList()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (attn_type_level_scatter_dict): TypeLevelScatterModuleDict()\n",
      ")\n",
      "torch.Size([55, 56])\n",
      "torch.Size([12, 60])\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import e3nn\n",
    "from e3nn.o3 import Irreps\n",
    "\n",
    "# edge_a, edge_b: \"6-8\", \"8-6\"\n",
    "# node_a, node_b: \"6\", \"8\"\n",
    "\n",
    "irreps_nodes_a: Irreps = Irreps(\"16x0e+8x1e+4x2e\")\n",
    "irreps_nodes_b: Irreps = Irreps(\"16x0e+8x1e+4x2e\")\n",
    "irreps_node_dict: Dict[str, Irreps] = {\"6\": irreps_nodes_a, \"8\": irreps_nodes_b}\n",
    "irreps_edges_a: Irreps = Irreps(\"32x0e+16x1e\")\n",
    "irreps_edges_b: Irreps = Irreps(\"8x0e+16x1e\")\n",
    "irreps_edge_dict: Dict[str, Irreps] = {\"6-8\": irreps_edges_a, \"8-6\": irreps_edges_b}\n",
    "irreps_edge_length: Irreps = Irreps(\"16x0e\") # should only contain Irrep=\"0e\"\n",
    "irreps_edge_vec: Irreps = Irreps(\"8x0e+4x1e+4x2e+4x3e\")\n",
    "# irreps_edge_length_dict = {\"6-8\": irreps_edge_length, \"8-6\": irreps_edge_length}\n",
    "\n",
    "num_nodes_a: int = 6\n",
    "num_nodes_b: int = 12\n",
    "num_nodes_dict: Dict[str, int] = {\"6\": num_nodes_a, \"8\": num_nodes_b}\n",
    "edge_index_a: Tensor = torch.tensor([[x, y] for x in range(num_nodes_a) for y in range(num_nodes_b-1)]).T.contiguous()\n",
    "edge_index_b: Tensor = torch.tensor([[y, x] for x in range(num_nodes_a-1) for y in range(num_nodes_b-1)]).T.contiguous()\n",
    "# nearly fully connected, last \"6\" is isolated in \"8-6\", last \"8\" is isolated in both.\n",
    "edge_index_dict = {\"6-8\": edge_index_a, \"8-6\": edge_index_b} \n",
    "\n",
    "num_edges_a: int = edge_index_a.shape[1]\n",
    "num_edges_b: int = edge_index_b.shape[1]\n",
    "num_heads: int = 4\n",
    "\n",
    "# = parameter 0 = \n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "edge_fea_a = irreps_edges_a.randn(num_edges_a, -1, normalization=\"component\")\n",
    "edge_fea_b = irreps_edges_b.randn(num_edges_b, -1, normalization=\"component\")\n",
    "edge_fea_dict = {\"6-8\": edge_fea_a, \"8-6\": edge_fea_b}\n",
    "edge_vec_a = irreps_edge_vec.randn(num_edges_a, -1, normalization=\"component\")\n",
    "edge_vec_b = torch.randn(1) * irreps_edge_vec.randn(num_edges_b, -1, normalization=\"component\") \n",
    "edge_vec_dict = {\"6-8\": edge_vec_a, \"8-6\": edge_vec_b}\n",
    "edge_length_a = irreps_edge_length.randn(num_edges_a, -1, normalization=\"component\")\n",
    "edge_length_b = torch.randn(1) * irreps_edge_length.randn(num_edges_b, -1, normalization=\"component\")\n",
    "edge_length_dict = {\"6-8\": edge_length_a, \"8-6\": edge_length_b}\n",
    "node_fea_a = irreps_nodes_a.randn(num_nodes_a, -1, normalization=\"component\")\n",
    "node_fea_b = torch.randn(1) * irreps_nodes_b.randn(num_nodes_b, -1, normalization=\"component\")\n",
    "node_fea_dict = {\"6\": node_fea_a, \"8\": node_fea_b}\n",
    "\n",
    "from modelname.nn.moduledict import TransformerModuleDict\n",
    "net_7 = TransformerModuleDict(\n",
    "        irreps_node_fea_dict=irreps_node_dict, \n",
    "        irreps_edge_fea_dict=irreps_edge_dict, \n",
    "        irreps_edge_vec_embed=irreps_edge_vec, \n",
    "        irreps_edge_length_embed=irreps_edge_length,\n",
    "        num_heads=num_heads, \n",
    "        alpha_dropout=0.0\n",
    "    )\n",
    "\n",
    "# = parameter 1 = \n",
    "net = net_7 # net_7 \n",
    "print(net) # Note that non-scalars have no bias even bias=True\n",
    "\n",
    "if isinstance(net, TransformerModuleDict):\n",
    "    out_edge, out_node = net(\n",
    "        edge_fea_dict=edge_fea_dict, \n",
    "        node_fea_dict=node_fea_dict, \n",
    "        edge_index_dict=edge_index_dict, \n",
    "        edge_vec_embed_dict=edge_vec_dict, \n",
    "        edge_length_embed_dict=edge_length_dict,\n",
    "        num_nodes_dict=num_nodes_dict\n",
    "    )\n",
    "\n",
    "# = parameter 2 =\n",
    "key_edge: str = \"8-6\" # \"6-8\" or \"8-6\" \n",
    "key_node: str = \"8\" # \"6\" or \"8\" \n",
    "print(out_edge[key_edge].shape)\n",
    "print(out_node[key_node].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1334,  0.0134,  0.9910],\n",
      "        [-0.5643, -0.8230, -0.0649],\n",
      "        [ 0.8147, -0.5678,  0.1174]])\n",
      "Is TransformerModuleDict rotation equivariant?\n",
      "Max Error: 1.4007091522216797e-06.\n",
      "Mean Error (L1Loss): 8.836180853677433e-08.\n",
      "(Min Absolute of out_1: 3.4095199225703254e-05\n",
      "Min Absolute of out_2: 3.4074535506078973e-05\n",
      "Max Absolute of out_1: 1.0643510818481445\n",
      "Max Absolute of out_2: 1.0643517971038818\n",
      "Mean Absolute Value out_1: 0.10497470200061798\n",
      "Mean Absolute Value of out_2: 0.10497473180294037)\n"
     ]
    }
   ],
   "source": [
    "def outs_rot_invariance(rot: Tensor, \n",
    "                  model: torch.nn.Module, \n",
    "\n",
    "                  irreps_node_dict: Dict[str, Irreps], \n",
    "                  irreps_edge_dict: Dict[str, Irreps],\n",
    "                  irreps_edge_vec: Irreps,\n",
    "\n",
    "                  edge_fea_dict: Dict[str, Tensor],\n",
    "                  node_fea_dict: Dict[str, Tensor],\n",
    "                  edge_index_dict: Dict[str, Tensor],\n",
    "                  edge_vec_dict: Dict[str, Tensor],\n",
    "                  edge_length_dict: Dict[str, Tensor],\n",
    "                  num_nodes_dict: Dict[str, int],\n",
    "                  ):\n",
    "    \"\"\"Unit test for checking whether a model (GNN model/layer) is \n",
    "    rotation equivariant.\n",
    "    \"\"\"\n",
    "    # Rotate Output\n",
    "    if isinstance(model, TransformerModuleDict):\n",
    "        out_edge_1, out_node_1 = model(\n",
    "            edge_fea_dict=edge_fea_dict, \n",
    "            node_fea_dict=node_fea_dict, \n",
    "            edge_index_dict=edge_index_dict, \n",
    "            edge_vec_embed_dict=edge_vec_dict, \n",
    "            edge_length_embed_dict=edge_length_dict,\n",
    "            num_nodes_dict=num_nodes_dict\n",
    "        )\n",
    "        \n",
    "    for edge, v in out_edge_1.items():\n",
    "        irreps_out: Irreps = irreps_edge_dict[edge]\n",
    "        D_out = irreps_out.D_from_matrix(rot)\n",
    "        out_edge_1[edge] = v @ D_out.T\n",
    "    for node, v in out_node_1.items():\n",
    "        irreps_out: Irreps = irreps_node_dict[node]\n",
    "        D_out = irreps_out.D_from_matrix(rot)\n",
    "        out_node_1[node] = v @ D_out.T\n",
    "\n",
    "    # Rotate Input\n",
    "    new_edge_fea_dict: Dict[str, Tensor] = {}\n",
    "    new_node_fea_dict: Dict[str, Tensor] = {}\n",
    "    new_edge_vec_dict: Dict[str, Tensor] = {}\n",
    "    for edge, v in edge_fea_dict.items():\n",
    "        irreps_in: Irreps = irreps_edge_dict[edge]\n",
    "        D_in = irreps_in.D_from_matrix(rot)\n",
    "        new_edge_fea_dict[edge] = v @ D_in.T \n",
    "    for node, v in node_fea_dict.items():\n",
    "        irreps_in: Irreps = irreps_node_dict[node]\n",
    "        D_in = irreps_in.D_from_matrix(rot)\n",
    "        new_node_fea_dict[node] = v @ D_in.T\n",
    "    for edge, v in edge_vec_dict.items():\n",
    "        irreps_in: Irreps = irreps_edge_vec\n",
    "        D_in = irreps_in.D_from_matrix(rot)\n",
    "        new_edge_vec_dict[edge] = v @ D_in.T \n",
    "\n",
    "    # Forward pass on rotated example\n",
    "    if isinstance(model, TransformerModuleDict):\n",
    "        out_edge_2, out_node_2 = model(\n",
    "            edge_fea_dict=new_edge_fea_dict, \n",
    "            node_fea_dict=new_node_fea_dict, \n",
    "            edge_index_dict=edge_index_dict, \n",
    "            edge_vec_embed_dict=new_edge_vec_dict, \n",
    "            edge_length_embed_dict=edge_length_dict,\n",
    "            num_nodes_dict=num_nodes_dict\n",
    "        )\n",
    "\n",
    "    return out_edge_1, out_node_1, out_edge_2, out_node_2\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "rot = e3nn.o3.rand_matrix()\n",
    "print(rot)\n",
    "\n",
    "out_edge_1, out_node_1, out_edge_2, out_node_2 = outs_rot_invariance(\n",
    "    rot=rot, \n",
    "    model=net, \n",
    "    irreps_node_dict=irreps_node_dict, \n",
    "    irreps_edge_dict=irreps_edge_dict,\n",
    "    irreps_edge_vec=irreps_edge_vec,\n",
    "    edge_fea_dict=edge_fea_dict,\n",
    "    node_fea_dict=node_fea_dict,\n",
    "    edge_index_dict=edge_index_dict,\n",
    "    edge_vec_dict=edge_vec_dict,\n",
    "    edge_length_dict=edge_length_dict,\n",
    "    num_nodes_dict=num_nodes_dict,\n",
    ")\n",
    "\n",
    "# = parameter 3 = \n",
    "# Do not forget to alpha_drop=0.0 when test equivariance !\n",
    "# Edge \n",
    "out_1 = out_edge_1[key_edge]\n",
    "out_2 = out_edge_2[key_edge]\n",
    "# Node\n",
    "\"\"\"\n",
    "out_1 = out_node_1[key_node]\n",
    "out_2 = out_node_2[key_node]\n",
    "\"\"\"\n",
    "min_out_1 = out_1.abs().min()\n",
    "min_out_2 = out_2.abs().min()\n",
    "max_out_1 = out_1.abs().max()\n",
    "max_out_2 = out_2.abs().max()\n",
    "ave_out_1 = out_1.abs().mean()\n",
    "ave_out_2 = out_2.abs().mean()\n",
    "\n",
    "error = out_2 - out_1\n",
    "max_error = error.abs().max()\n",
    "ave_error = error.abs().mean()\n",
    "\n",
    "print(f\"Is {type(net).__name__} rotation equivariant?\")\n",
    "print(f\"Max Error: {max_error}.\\n\"\n",
    "      f\"Mean Error (L1Loss): {ave_error}.\\n\"\n",
    "      f\"(\"\n",
    "      f\"Min Absolute of out_1: {min_out_1}\\n\"\n",
    "      f\"Min Absolute of out_2: {min_out_2}\\n\"\n",
    "      f\"Max Absolute of out_1: {max_out_1}\\n\"\n",
    "      f\"Max Absolute of out_2: {max_out_2}\\n\"\n",
    "      f\"Mean Absolute Value out_1: {ave_out_1}\\n\"\n",
    "      f\"Mean Absolute Value of out_2: {ave_out_2})\"\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeph_1107",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
